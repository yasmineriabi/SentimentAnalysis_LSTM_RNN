{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "O2vFP7N9Lr5H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HX2DUcplawqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c452d787-ed96-486a-e684-35995812a47d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Explore Dataset**"
      ],
      "metadata": {
        "id": "Q7srHcPEMALK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "df.head()\n",
        "\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(df['sentiment'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pwo7LJTBL5qA",
        "outputId": "c15863d1-657a-4648-b27f-d3e428f458d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 50000\n",
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**"
      ],
      "metadata": {
        "id": "ZiO7imj_MRwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", '', text)\n",
        "    text = re.sub(r\"\\s+\", ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def tokenize(text):\n",
        "    words = [w for w in text.split() if w not in stop_words]\n",
        "    return words\n",
        "\n",
        "df['review'] = df['review'].apply(preprocess_text)\n",
        "df['tokens'] = df['review'].apply(tokenize)\n"
      ],
      "metadata": {
        "id": "IpUy1InNMX6T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vocabulary and Encoding**"
      ],
      "metadata": {
        "id": "Po1Rml7YMbwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Build vocabulary\n",
        "all_words = [word for tokens in df['tokens'] for word in tokens]\n",
        "word_counts = Counter(all_words)\n",
        "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "vocab_to_int = {word: idx + 1 for idx, word in enumerate(vocab)}  # +1 to reserve 0 for padding\n",
        "\n",
        "def encode_tokens(tokens):\n",
        "    return [vocab_to_int[word] for word in tokens if word in vocab_to_int]\n",
        "\n",
        "df['encoded'] = df['tokens'].apply(encode_tokens)\n"
      ],
      "metadata": {
        "id": "eaL_IuVtMhSL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Padding**"
      ],
      "metadata": {
        "id": "rDpqYiYAMmZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_features(reviews, seq_length=500):\n",
        "    features = np.zeros((len(reviews), seq_length), dtype=int)\n",
        "    for i, row in enumerate(reviews):\n",
        "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "    return features\n",
        "\n",
        "features = pad_features(df['encoded'], 500)\n",
        "labels = np.array([1 if label == 'positive' else 0 for label in df['sentiment']])\n"
      ],
      "metadata": {
        "id": "SSOujumIMohB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train/Test Split**"
      ],
      "metadata": {
        "id": "F0yQ9ypmMsun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "G87etQoJM2cC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Simple RNN**"
      ],
      "metadata": {
        "id": "h7J88CIuM6bO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim=1):\n",
        "        super(SentimentRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.rnn(x)\n",
        "        out = self.fc(out[:, -1])\n",
        "        return self.sig(out)\n"
      ],
      "metadata": {
        "id": "N1CtWRHDND1b"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM**"
      ],
      "metadata": {
        "id": "G3vl1U9QNGuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim=1):\n",
        "        super(SentimentLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, (hidden, cell) = self.lstm(x)\n",
        "        out = self.fc(out[:, -1])\n",
        "        return self.sig(out)\n"
      ],
      "metadata": {
        "id": "2kRyXjB0NNjk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Function**"
      ],
      "metadata": {
        "id": "K4NXICp_NTHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, epochs=3):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            output = model(inputs).squeeze()\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "dFoYo4EPNW3s"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation Function**"
      ],
      "metadata": {
        "id": "r-FGbNBzNgL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
        "            output = model(inputs).squeeze()\n",
        "            preds = torch.round(output)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
      ],
      "metadata": {
        "id": "k3axJE7cNlgz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Both Models**"
      ],
      "metadata": {
        "id": "GbWxUHn-Npxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab_to_int) + 1\n",
        "embed_dim = 128\n",
        "hidden_dim = 128\n",
        "\n",
        "# RNN\n",
        "rnn_model = SentimentRNN(vocab_size, embed_dim, hidden_dim)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001)\n",
        "train_model(rnn_model, train_loader, criterion, optimizer, epochs=3)\n",
        "evaluate_model(rnn_model, test_loader)\n",
        "\n",
        "# LSTM\n",
        "lstm_model = SentimentLSTM(vocab_size, embed_dim, hidden_dim)\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "train_model(lstm_model, train_loader, criterion, optimizer, epochs=3)\n",
        "evaluate_model(lstm_model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe_BzCwzNsli",
        "outputId": "9a8d80e2-e715-4547-efb2-0b95022ab37c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.6195\n",
            "Epoch 2/3, Loss: 0.5442\n",
            "Epoch 3/3, Loss: 0.4579\n",
            "Test Accuracy: 78.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict New Reviews**"
      ],
      "metadata": {
        "id": "WFxcfvoHNx_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_review(model, text):\n",
        "    model.eval()\n",
        "    tokens = tokenize(preprocess_text(text))\n",
        "    encoded = encode_tokens(tokens)\n",
        "    padded = pad_features([encoded])\n",
        "    input_tensor = torch.from_numpy(padded).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "    pred = torch.round(output).item()\n",
        "    return \"Positive\" if pred == 1 else \"Negative\"\n",
        "\n",
        "sample = \"The movie was really touching and well-acted!\"\n",
        "print(predict_review(rnn_model, sample))\n"
      ],
      "metadata": {
        "id": "XQ2u5yqgN0aT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}